{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Part 4 - Achieving Privacy with Private Aggregation of Teacher Ensembles.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltgrGx6xpQg7",
        "colab_type": "code",
        "outputId": "2ab35fc8-22e5-4c7b-9214-9a0ef0d0ce81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUsY4znI4eiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install syft"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB-TXvI0pVbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import optim\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision import transforms\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAdR_-h8pQhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_exp_1 = \"/content/drive/My Drive//Raw_Data/experiment-i/\"\n",
        "positions = [\"justAPlaceholder\",\"supine\", \"right\", \"left\", \"right\", \"right\", \"left\", \"left\", \"supine\", \"supine\", \"supine\", \"supine\", \"supine\", \"right\", \"left\", \"supine\", \"supine\", \"supine\"]\n",
        "\n",
        "subjects_dict = {}\n",
        "\n",
        "\n",
        "Normalize = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor()\n",
        "  ])\n",
        "\n",
        "for _, dirs, _ in os.walk(path_exp_1):\n",
        "  for directory in dirs:\n",
        "    # each directory is a subject\n",
        "    subject = directory\n",
        "    data = None\n",
        "    labels = None\n",
        "    \n",
        "    for _, _, files in os.walk(os.path.join(path_exp_1, directory)):\n",
        "      for file in files:\n",
        "        file_path = os.path.join(path_exp_1, directory, file)\n",
        "        with open(file_path, 'r') as f:\n",
        "          # Start from second recording, as the first two are corrupted\n",
        "          for line in f.read().splitlines()[2:]:\n",
        "            def token_position(x):\n",
        "              return {\n",
        "                'supine': 0,\n",
        "                'left': 1,\n",
        "                'right': 2,\n",
        "                'left_fetus': 1,\n",
        "                'right_fetus': 2\n",
        "              }[x]\n",
        "            \n",
        "            \n",
        "            raw_data = np.fromstring(line, dtype=float, sep='\\t')\n",
        "            file_data = np.round(raw_data*255/1000).astype(np.uint8) # Change the range from [0-1000] to [0-255]. This allows us to use tranforms later.\n",
        "            file_data = Normalize(file_data.reshape(64,32))\n",
        "            file_data = file_data.view(1, 64, 32)\n",
        "            file_label = token_position(positions[int(file[:-4])]) # Turn the file index into position list, and turn position list into reduced indices.\n",
        "            file_label = np.array([file_label])\n",
        "            \n",
        "            if data is None:\n",
        "              data = file_data\n",
        "            else:\n",
        "              data = np.concatenate((data, file_data), axis=0)\n",
        "\n",
        "            if labels is None:\n",
        "              labels = file_label\n",
        "            else:\n",
        "              labels = np.concatenate((labels, file_label), axis=0)\n",
        "              \n",
        "    subjects_dict[subject] = (torch.from_numpy(data), torch.from_numpy(labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "397KnMbXHjmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_exp_2 = \"/content/drive/My Drive/Raw_Data/experiment-ii/\"\n",
        "\n",
        "positions = {\n",
        "    \"B\":\"supine\",\n",
        "    \"1\":\"supine\",\n",
        "    \"C\":\"right\",\n",
        "    \"D\":\"left\",\n",
        "    \"E1\":\"right\",\n",
        "    \"E2\":\"right\",\n",
        "    \"E3\":\"left\",\n",
        "    \"E4\":\"left\",\n",
        "    \"E5\":\"right\",\n",
        "    \"E6\":\"left\",\n",
        "    \"F\":\"supine\",\n",
        "    \"G1\":\"supine\",\n",
        "    \"G2\":\"right\",\n",
        "    \"G3\":\"left\"\n",
        "}\n",
        "\n",
        "def token_position(x):\n",
        "  return {\n",
        "    'supine': 0,\n",
        "    'left': 1,\n",
        "    'right': 2\n",
        "  }[x]\n",
        "\n",
        "subjects_dict_air = {}\n",
        "subjects_dict_spo = {}\n",
        "\n",
        "def resize_and_rotate(image):\n",
        "  To_PIL_and_Resize = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((64, 32))\n",
        "  ])\n",
        "  rotated = TF.rotate(To_PIL_and_Resize(image), angle=180)\n",
        "  return transforms.ToTensor()(rotated)\n",
        "\n",
        "\n",
        "# each directory is a subject\n",
        "for _, subject_dirs, _ in os.walk(path_exp_2):\n",
        "  for subject in subject_dirs:\n",
        "    data = None\n",
        "    labels = None\n",
        "    \n",
        "    # each directory is a matresss\n",
        "    for _, mat_dirs, _ in os.walk(os.path.join(path_exp_2, subject)):\n",
        "      for mat in mat_dirs:\n",
        "        for _, _, files in os.walk(os.path.join(path_exp_2, subject, mat)):\n",
        "\n",
        "          for file in files:\n",
        "\n",
        "            file_path = os.path.join(path_exp_2, subject, mat, file)\n",
        "            raw_data = np.loadtxt(file_path)\n",
        "            file_data = np.round(raw_data*255/500).astype(np.uint8) # Change the range from [0-500] to [0-255]. This allows us to use tranforms later. \n",
        "            file_data = resize_and_rotate(file_data)\n",
        "            file_data = file_data.view(1, 64, 32)\n",
        "            \n",
        "            if file[-6] == \"E\" or file[-6] == \"G\":\n",
        "              file_label = positions[file[-6:-4]]\n",
        "            else:\n",
        "              file_label = positions[file[-6]]\n",
        "            \n",
        "            file_label = token_position(file_label)\n",
        "            file_label = np.array([file_label])           \n",
        "\n",
        "            if data is None:\n",
        "              data = file_data\n",
        "            else:\n",
        "              data = np.concatenate((data, file_data), axis=0)\n",
        "\n",
        "            if labels is None:\n",
        "              labels = file_label\n",
        "            else:\n",
        "              labels = np.concatenate((labels, file_label), axis=0)\n",
        "        \n",
        "        if mat == \"Air_Mat\":                        \n",
        "          subjects_dict_air[subject] = (torch.from_numpy(data), torch.from_numpy(labels))\n",
        "        else:\n",
        "          subjects_dict_spo[subject] = (torch.from_numpy(data), torch.from_numpy(labels))\n",
        "        \n",
        "        data = None\n",
        "        labels = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz97Vl2IpQhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datasets = {\n",
        "    \"Base\":subjects_dict,\n",
        "    \"Spo\":subjects_dict_spo,\n",
        "    \"Air\":subjects_dict_air\n",
        "}\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class Mat_Dataset(Dataset):\n",
        "  def __init__(self, mats, Subject_IDs):\n",
        "    \n",
        "    self.samples = []\n",
        "    self.labels = []\n",
        "    \n",
        "    for mat in mats:\n",
        "      data = datasets[mat]\n",
        "      self.samples.append(np.vstack([data.get(key)[0] for key in Subject_IDs]))\n",
        "      self.labels.append(np.hstack([data.get(key)[1] for key in Subject_IDs]))\n",
        "      \n",
        "    self.samples = np.vstack(self.samples)\n",
        "    self.labels = np.hstack(self.labels)\n",
        "    \n",
        "  def __len__(self):\n",
        "    return self.samples.shape[0]\n",
        "      \n",
        "  def __getitem__(self, idx):\n",
        "    return self.samples[idx], self.labels[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n8TBLJlpQhk",
        "colab_type": "text"
      },
      "source": [
        "PyTorch gives you the freedom to pretty much do anything with the Dataset class so long as you override two of the subclass functions:\n",
        "\n",
        "+ the __len__ function which returns the size of the dataset, and\n",
        "+ the __getitem__ function which returns a sample from the dataset given an index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1M2tm0_pQhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "data_exp_1 = Mat_Dataset([\"Base\"], [key for key in subjects_dict.keys()])\n",
        "\n",
        "trainset_exp_1, valset_exp_1 = random_split(data_exp_1, [round(len(data_exp_1)*0.70), len(data_exp_1) - round(len(data_exp_1)*0.70)])\n",
        "\n",
        "trainloader = DataLoader(trainset_exp_1, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(valset_exp_1, batch_size=64, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2vMiFmAJkoc",
        "colab_type": "code",
        "outputId": "f11644d4-d108-48f7-91cf-b3be8a8f4e5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        #Input channels = 1, output channels = 6\n",
        "        self.conv1 = torch.nn.Conv2d(1, 6, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = torch.nn.Conv2d(6, 18, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        \n",
        "        \n",
        "        # Inputs to hidden layer linear transformation\n",
        "        self.h1 = nn.Linear(18 * 16 * 8, 392)\n",
        "        self.h2 = nn.Linear(392, 98)\n",
        "        \n",
        "        # Output layer, 3 neurons - one for each position\n",
        "        self.output = nn.Linear(98, 3)\n",
        "        \n",
        "        # Define sigmoid activation and softmax output \n",
        "        self.relu = nn.ReLU()\n",
        "        self.logsoftmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        # Add a \"channel dimension\"\n",
        "        x = x.unsqueeze(1)\n",
        "        \n",
        "        #Computes the activation of the first convolution\n",
        "        #Size changes from (1, 64, 32) to (6, 64, 32)\n",
        "        \n",
        "        x = F.relu(self.conv1(x))\n",
        "\n",
        "        # LOOK HERE\n",
        "        #Size changes from (6, 32, 16) to (6, 32, 16)\n",
        "        x = self.pool(x)\n",
        "\n",
        "        \n",
        "        #Size changes from (6, 32, 16) to (18, 32, 16)\n",
        "        x = F.relu(self.conv2(x))\n",
        "\n",
        "        \n",
        "        #Size changes from (18, 32, 16) to (18, 16, 8)\n",
        "        x = self.pool(x)\n",
        "        \n",
        "        \n",
        "        #Reshape data to input to the input layer of the neural net\n",
        "        #Size changes from (18, 16, 16) to (1, 4608)\n",
        "        \n",
        "        x = x.view(x.shape[0], -1)\n",
        "        # Pass the input tensor through each of our operations\n",
        "        x = self.h1(x)\n",
        "\n",
        "        x = self.relu(x)\n",
        "        x = self.h2(x)\n",
        "\n",
        "        x = self.relu(x)\n",
        "        x = self.output(x)\n",
        "\n",
        "        x = self.logsoftmax(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Network(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(6, 18, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (h1): Linear(in_features=2304, out_features=392, bias=True)\n",
              "  (h2): Linear(in_features=392, out_features=98, bias=True)\n",
              "  (output): Linear(in_features=98, out_features=3, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (logsoftmax): LogSoftmax()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUrBVpHn0LY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(device, trainloader, testloader=None):\n",
        "  model = Network()\n",
        "\n",
        "  criterion = nn.NLLLoss()\n",
        "\n",
        "  optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  epochs = 15\n",
        "  running_loss = 0\n",
        "\n",
        "  torch.manual_seed(123)\n",
        "\n",
        "  train_losses, test_losses = [], []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    for inputs, labels in trainloader:\n",
        "\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      logps = model.forward(inputs)\n",
        "      loss = criterion(logps, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    else:\n",
        "\n",
        "      if testloader != None:\n",
        "        \n",
        "        test_loss = 0\n",
        "        accuracy = 0\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            logps = model.forward(inputs)\n",
        "            test_loss += criterion(logps, labels)\n",
        "\n",
        "            ps = torch.exp(logps)\n",
        "            top_p, top_class = ps.topk(1, dim=1)\n",
        "            equals = top_class == labels.view(*top_class.shape)\n",
        "            accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "        train_losses.append(running_loss/len(trainloader))\n",
        "        test_losses.append(test_loss/len(testloader))\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "              f\"Train loss: {running_loss/len(trainloader):.3f}.. \"\n",
        "              f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
        "              f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
        "        running_loss = 0\n",
        "        model.train()\n",
        "\n",
        "\n",
        "  plt.plot(train_losses, label='Training loss')\n",
        "  #plt.plot(test_losses, label='Validation loss')\n",
        "  plt.legend(frameon=False)\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6SY6Tt9YW1U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "outputId": "822c10ba-81d7-43d1-e558-d7aa1e150149"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Working on: {device.type}\")\n",
        "\n",
        "hospitals = [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\", \"S6\", \"S7\", \"S8\", \"S9\", \"S10\"]\n",
        "\n",
        "hospitals_preds = {}\n",
        "\n",
        "my_hospital = [\"S11\", \"S12\", \"S13\"]\n",
        "\n",
        "my_data = Mat_Dataset([\"Base\"], my_hospital)\n",
        "my_data_loader = DataLoader(my_data, batch_size=len(my_data), shuffle=False)\n",
        "\n",
        "for hospital in hospitals:\n",
        "  private_data = Mat_Dataset([\"Base\"], [hospital])\n",
        "  private_data_loader = DataLoader(private_data, batch_size=64, shuffle=True)\n",
        "  private_model = train_model(device, private_data_loader)\n",
        "  \n",
        "  accuracy = 0\n",
        "  private_model.to(device)\n",
        "  private_model.eval()\n",
        "  with torch.no_grad():\n",
        "      \n",
        "    for inputs, labels in my_data_loader:\n",
        "\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "      logps = private_model.forward(inputs)\n",
        "      ps = torch.exp(logps)\n",
        "      top_p, top_class = ps.topk(1, dim=1)\n",
        "      equals = top_class == labels.view(*top_class.shape)\n",
        "      accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "      print(f\"Accuracy {accuracy/len(my_data_loader):.3f}\")\n",
        "      hospitals_preds[hospital] = top_class\n",
        "    \n",
        "  "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Working on: cuda\n",
            "Accuracy 0.828\n",
            "Accuracy 0.875\n",
            "Accuracy 0.917\n",
            "Accuracy 0.738\n",
            "Accuracy 0.837\n",
            "Accuracy 0.869\n",
            "Accuracy 0.816\n",
            "Accuracy 0.857\n",
            "Accuracy 0.827\n",
            "Accuracy 0.882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntDAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FpVAOIChuHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Images x Hospitals\n",
        "\n",
        "#hospital_p.shape\n",
        "\n",
        "hospital_p = np.hstack([hospitals_preds.get(key).cpu() for key in hospitals_preds.keys()])\n",
        "num_positions = 3\n",
        "\n",
        "def hospitals_vote(hospital_p, epsilon=0.1):\n",
        "  new_labels = list()\n",
        "\n",
        "  for a_measurement in hospital_p:\n",
        "\n",
        "    label_counts = np.bincount(a_measurement, minlength=num_positions)\n",
        "\n",
        "    beta = 1 / epsilon\n",
        "\n",
        "    for i in range(len(label_counts)):\n",
        "      label_counts[i] += np.random.laplace(0, beta, 1)\n",
        "\n",
        "    new_label = np.argmax(label_counts)\n",
        "\n",
        "    new_labels.append(new_label)\n",
        "  \n",
        "  return np.array(new_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBfwMnH91RQo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b8768132-ee66-4a6a-f412-9269162342fd"
      },
      "source": [
        ""
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4324, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-DMo2MmnLRL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "18fc9918-b344-4f29-ee80-bdc1fa97ae9a"
      },
      "source": [
        "from syft.frameworks.torch.differential_privacy import pate\n",
        "\n",
        "indices = hospitals_vote(hospital_p, epsilon=0.01)\n",
        "data_dep_eps, data_ind_eps = pate.perform_analysis(teacher_preds=hospital_p.transpose(), indices=indices, noise_eps=0.01, delta=1e-5)\n",
        "print(\"Data Independent Epsilon:\", data_ind_eps)\n",
        "print(\"Data Dependent Epsilon:\", data_dep_eps)\n",
        "\n",
        "sum(np.array(indices) == tr_d)/np.shape(np.array(indices) == tr_d)[0]"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Independent Epsilon: 7.202231366242557\n",
            "Data Dependent Epsilon: 7.202231366242335\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3503700277520814"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLlDG9tNw-K3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# True\n",
        "tr_d = my_data[:][1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvy4Lrtf2UtH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "07755846-bc94-4b8a-c140-9fc235167345"
      },
      "source": [
        "indices = hospitals_vote(hospital_p, epsilon=0.5)\n",
        "sum(np.array(indices) == tr_d)/np.shape(np.array(indices) == tr_d)[0]"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8896854764107308"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hHYcRha2WK4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9da89ea6-ffed-421a-a468-6edd1287e130"
      },
      "source": [
        "np.random.laplace(0, 1/0.1, 1)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-13.52000765])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ9XEJsA6I3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}